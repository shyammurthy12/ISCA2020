\section{Performance Impact}
There are a few reasons for change in peformance with the changes we introduce. Firstly, we affect the placement of blocks in the cache, which affects the number of cache misses. We have extra logic to compute the new set index from the address bits, for all the three design schemes proposed. This has the potential to introduce additional latency before we can access the L1 cache, and could affect performance. For the last scheme, we have a table containing the hash scheme that has to be looked up, before we can compute the modified index. This has the potential to add extra latency before we can access the L1 cache.   
\subsection {Impact on Cache Misses}
We measure the impact on number of cache misses by measuring the MPKI as well as miss rates for different applications. The MPKI improves a lot for \textit{astar} as well as \textit{GemsFDTD}. This is not unexpected, as was studied by Seznec et.al \cite{skewed}, allowing placement of blocks at different locations in the cache. It has the benefit of adding more associativity and bringing down the number of conflict misses. To validate this assumption of ours, we also run the applications with a fully associative cache, to understand how much benefit the different applications see from bringing down all conflict misses. The same applications, namely \textit{astar} and \textit{GemsFDTD}, see most benefit from moving to a fully associative cache and the other applications are negligibly impacted by increase in associativity, which ties in well with the observations we made with our cache placement strategy.   
\subsection {Storage Overheads} <table storage>
Size of each tag goes up by 9 bits, which is the size of a tag in a fully associative cache. This allows us to reap the performance benefit of reduced conflict misses as well as the security benefit of having a single set and no information leaking from the knowledge about an eviction. For a 64kB cache with 2 ways of associativity, this would be a total of around 512B of extra storage.    
For our third scheme, we have a table that is indexed using the lower bits of the page number. 
\subsection {Impact on Latency} <table lookup latency and hash computation latency>
For the third design, we have a table lookup to figure out the hashing scheme that is to be used. The latency of this table lookup can be hidden by overlapping it with the process of address generation. The contents of the base register can be used to extract the virtual page number, whose lower bits are used to index the table. There could be an overflow when adding the base and offset register, however the access would just go to the next virtual page in this case. One can simply use the scheme corresponding to page number (VPN) obtained from the base register and VPN+1. Then select one of the schemes based on whether there was an overflow or not, when adding the base register with the offset.
For all the three designs, the commonality is that we have to XOR different set of bits from address with each other to obtain the modified set index. This incurs extra latency, <latch logic to tolerate this additional latency> 
\subsection {Energy and Power}
\subsection{Hotspot mitigation}
\subsection{Sensitivity to associativity}

